{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caedc61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": None,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Test cell to confirm kernel execution\\n\",\n",
    "    \"import sys\\n\",\n",
    "    \"print(f'Python version: {sys.version}')\\n\",\n",
    "    \"print('Kernel execution started')\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": None,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import os\\n\",\n",
    "    \"import torch\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import subprocess\\n\",\n",
    "    \"import sys\\n\",\n",
    "    \"import logging\\n\",\n",
    "    \"from pathlib import Path\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Set up logging\\n\",\n",
    "    \"logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\\n\",\n",
    "    \"logger = logging.getLogger(__name__)\\n\",\n",
    "    \"logger.info(\\\"Starting kernel execution\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Install dependencies with error handling\\n\",\n",
    "    \"def install_packages():\\n\",\n",
    "    \"    packages = [\\n\",\n",
    "    \"        \\\"transformers==4.44.2\\\",\\n\",\n",
    "    \"        \\\"datasets==3.0.1\\\",\\n\",\n",
    "    \"        \\\"torch==2.4.1\\\",\\n\",\n",
    "    \"        \\\"evaluate==0.4.3\\\",\\n\",\n",
    "    \"        \\\"huggingface_hub==0.25.2\\\",\\n\",\n",
    "    \"        \\\"peft==0.13.2\\\",\\n\",\n",
    "    \"        \\\"psutil==6.0.0\\\"\\n\",\n",
    "    \"    ]\\n\",\n",
    "    \"    for pkg in packages:\\n\",\n",
    "    \"        try:\\n\",\n",
    "    \"            subprocess.check_call([sys.executable, \\\"-m\\\", \\\"pip\\\", \\\"install\\\", pkg, \\\"--quiet\\\"])\\n\",\n",
    "    \"            logger.info(f\\\"Successfully installed {pkg}\\\")\\n\",\n",
    "    \"        except subprocess.CalledProcessError as e:\\n\",\n",
    "    \"            logger.error(f\\\"Failed to install {pkg}: {e}\\\")\\n\",\n",
    "    \"            print(f\\\"Warning: Failed to install {pkg}. Proceeding with available packages.\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"install_packages()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Verify imports\\n\",\n",
    "    \"try:\\n\",\n",
    "    \"    from transformers import (\\n\",\n",
    "    \"        BertForMaskedLM,\\n\",\n",
    "    \"        RobertaForMaskedLM,\\n\",\n",
    "    \"        AutoTokenizer,\\n\",\n",
    "    \"        Trainer,\\n\",\n",
    "    \"        TrainingArguments,\\n\",\n",
    "    \"        DataCollatorForLanguageModeling\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    from datasets import load_dataset, Dataset\\n\",\n",
    "    \"    import evaluate\\n\",\n",
    "    \"    import time\\n\",\n",
    "    \"    import json\\n\",\n",
    "    \"    import psutil\\n\",\n",
    "    \"    from peft import LoraConfig, get_peft_model\\n\",\n",
    "    \"    import torch.nn.functional as F\\n\",\n",
    "    \"    logger.info(\\\"All imports successful\\\")\\n\",\n",
    "    \"except ImportError as e:\\n\",\n",
    "    \"    logger.error(f\\\"Import error: {e}\\\")\\n\",\n",
    "    \"    raise\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Setup\\n\",\n",
    "    \"input_dir = Path('/kaggle/input') if Path('/kaggle/input').exists() else Path('.')\\n\",\n",
    "    \"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\\n\",\n",
    "    \"quant_device = torch.device('cpu')  # Quantized models run on CPU\\n\",\n",
    "    \"logger.info(f'Using device: {device} for main models, {quant_device} for quantized models')\\n\",\n",
    "    \"torch.cuda.empty_cache()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Baseline memory\\n\",\n",
    "    \"baseline_memory = psutil.Process().memory_info().rss / 1024**2  # MB\\n\",\n",
    "    \"logger.info(f\\\"Baseline memory: {baseline_memory:.2f} MB\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load dataset\\n\",\n",
    "    \"dataset_path = os.environ.get('DATASET_PATH', '')\\n\",\n",
    "    \"if dataset_path:\\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        if dataset_path.endswith('.csv'):\\n\",\n",
    "    \"            dataset = load_dataset('csv', data_files=dataset_path)\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            dataset = load_dataset(dataset_path, split='train')\\n\",\n",
    "    \"        dataset = dataset.filter(lambda x: x['text'].strip() != '' and len(x['text'].split()) > 5)\\n\",\n",
    "    \"        logger.info(f\\\"Loaded dataset from {dataset_path}\\\")\\n\",\n",
    "    \"    except Exception as e:\\n\",\n",
    "    \"        logger.warning(f'Failed to load dataset {dataset_path}: {e}')\\n\",\n",
    "    \"        dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train')\\n\",\n",
    "    \"        dataset = dataset.filter(lambda x: x['text'].strip() != '' and len(x['text'].split()) > 5)\\n\",\n",
    "    \"        logger.info(\\\"Fell back to wikitext dataset\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        dataset_files = list(input_dir.glob('*.csv'))\\n\",\n",
    "    \"        if dataset_files:\\n\",\n",
    "    \"            dataset = load_dataset('csv', data_files=str(dataset_files[0]))\\n\",\n",
    "    \"            logger.info(f\\\"Loaded dataset from {dataset_files[0]}\\\")\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            raise FileNotFoundError('No CSV file found; falling back to wikitext.')\\n\",\n",
    "    \"        dataset = dataset.filter(lambda x: x['text'].strip() != '' and len(x['text'].split()) > 5)\\n\",\n",
    "    \"    except Exception:\\n\",\n",
    "    \"        dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train')\\n\",\n",
    "    \"        dataset = dataset.filter(lambda x: x['text'].strip() != '' and len(x['text'].split()) > 5)\\n\",\n",
    "    \"        logger.info(\\\"Fell back to wikitext dataset\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Log dataset details\\n\",\n",
    "    \"num_samples = len(dataset)\\n\",\n",
    "    \"logger.info(f\\\"Dataset size: {num_samples} samples\\\")\\n\",\n",
    "    \"sample_texts = dataset[:5]['text']\\n\",\n",
    "    \"logger.info(f\\\"Sample texts: {sample_texts}\\\")\\n\",\n",
    "    \"language = \\\"Tagalog\\\" if dataset_path.endswith('.csv') else \\\"English (wikitext)\\\"\\n\",\n",
    "    \"logger.info(f\\\"Dataset language: {language}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Classify dataset\\n\",\n",
    "    \"classification = 'small' if num_samples < 512 else 'big'\\n\",\n",
    "    \"data_type = 'low-resource NLP' if num_samples < 1000 else 'standard NLP'\\n\",\n",
    "    \"train_test_split = dataset.train_test_split(test_size=0.2, seed=42)\\n\",\n",
    "    \"train_dataset = train_test_split['train']\\n\",\n",
    "    \"val_dataset = train_test_split['test']\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Tokenization\\n\",\n",
    "    \"base_tokenizer = AutoTokenizer.from_pretrained('GKLMIP/bert-tagalog-base-uncased')\\n\",\n",
    "    \"improved_model_path = 'distilbert-base-uncased' if classification == 'small' else 'jcblaise/roberta-tagalog-base'\\n\",\n",
    "    \"improved_tokenizer = AutoTokenizer.from_pretrained(improved_model_path, do_lower_case=False)\\n\",\n",
    "    \"\\n\",\n",
    "    \"def tokenize(examples, tokenizer):\\n\",\n",
    "    \"    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=64)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Filter non-Tagalog tokens if using wikitext\\n\",\n",
    "    \"def filter_non_tagalog(examples, tokenizer):\\n\",\n",
    "    \"    if language != \\\"English (wikitext)\\\":\\n\",\n",
    "    \"        return True\\n\",\n",
    "    \"    tokens = tokenizer(examples['text'], truncation=True, max_length=64).input_ids\\n\",\n",
    "    \"    vocab = set(tokenizer.get_vocab().keys())\\n\",\n",
    "    \"    decoded = tokenizer.convert_ids_to_tokens(tokens)\\n\",\n",
    "    \"    return all(\\n\",\n",
    "    \"        token in vocab and \\n\",\n",
    "    \"        not token.startswith('##') and \\n\",\n",
    "    \"        all(ord(c) < 128 for c in tokenizer.decode([t]) if t not in [tokenizer.pad_token_id, tokenizer.cls_token_id, tokenizer.sep_token_id])\\n\",\n",
    "    \"        for t, token in zip(tokens, decoded)\\n\",\n",
    "    \"        if token not in [tokenizer.pad_token, tokenizer.cls_token, tokenizer.sep_token]\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"\\n\",\n",
    "    \"if language == \\\"English (wikitext)\\\":\\n\",\n",
    "    \"    train_dataset = train_dataset.filter(lambda x: filter_non_tagalog(x, base_tokenizer))\\n\",\n",
    "    \"    val_dataset = val_dataset.filter(lambda x: filter_non_tagalog(x, base_tokenizer))\\n\",\n",
    "    \"    logger.info(f\\\"Filtered dataset size: train={len(train_dataset)}, val={len(val_dataset)}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"tokenized_train_base = train_dataset.map(lambda x: tokenize(x, base_tokenizer), batched=True, remove_columns=['text'])\\n\",\n",
    "    \"tokenized_val_base = val_dataset.map(lambda x: tokenize(x, base_tokenizer), batched=True, remove_columns=['text'])\\n\",\n",
    "    \"tokenized_train_improved = train_dataset.map(lambda x: tokenize(x, improved_tokenizer), batched=True, remove_columns=['text'])\\n\",\n",
    "    \"tokenized_val_improved = val_dataset.map(lambda x: tokenize(x, improved_tokenizer), batched=True, remove_columns=['text'])\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Validate tokens\\n\",\n",
    "    \"def validate_tokens(dataset, tokenizer):\\n\",\n",
    "    \"    vocab = set(tokenizer.get_vocab().keys())\\n\",\n",
    "    \"    invalid_count = 0\\n\",\n",
    "    \"    for example in dataset:\\n\",\n",
    "    \"        tokens = tokenizer.convert_ids_to_tokens(example['input_ids'])\\n\",\n",
    "    \"        if any(token not in vocab for token in tokens if token not in [tokenizer.pad_token, tokenizer.cls_token, tokenizer.sep_token]):\\n\",\n",
    "    \"            invalid_count += 1\\n\",\n",
    "    \"    logger.info(f\\\"Invalid tokens found: {invalid_count}/{len(dataset)} samples\\\")\\n\",\n",
    "    \"    return invalid_count == 0\\n\",\n",
    "    \"\\n\",\n",
    "    \"logger.info(\\\"Validating base tokenizer tokens...\\\")\\n\",\n",
    "    \"base_tokens_valid = validate_tokens(tokenized_val_base, base_tokenizer)\\n\",\n",
    "    \"logger.info(\\\"Validating improved tokenizer tokens...\\\")\\n\",\n",
    "    \"improved_tokens_valid = validate_tokens(tokenized_val_improved, improved_tokenizer)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load models\\n\",\n",
    "    \"base_model = BertForMaskedLM.from_pretrained('GKLMIP/bert-tagalog-base-uncased').to(device)\\n\",\n",
    "    \"improved_model = RobertaForMaskedLM.from_pretrained(improved_model_path).to(device)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Apply LoRA for fine-tuning\\n\",\n",
    "    \"lora_config = LoraConfig(\\n\",\n",
    "    \"    r=8,\\n\",\n",
    "    \"    lora_alpha=16,\\n\",\n",
    "    \"    target_modules=[\\\"query\\\", \\\"value\\\"],\\n\",\n",
    "    \"    lora_dropout=0.1,\\n\",\n",
    "    \"    bias=\\\"none\\\"\\n\",\n",
    "    \")\\n\",\n",
    "    \"improved_model = get_peft_model(improved_model, lora_config)\\n\",\n",
    "    \"improved_model.print_trainable_parameters()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Fine-tuning\\n\",\n",
    "    \"fine_tuned_model_path = '/kaggle/working/fine_tuned_model' if Path('/kaggle/working').exists() else './fine_tuned_model'\\n\",\n",
    "    \"if os.path.exists(fine_tuned_model_path):\\n\",\n",
    "    \"    logger.info(f'Loading fine-tuned model from {fine_tuned_model_path}')\\n\",\n",
    "    \"    if classification == 'small':\\n\",\n",
    "    \"        from transformers import DistilBertForMaskedLM\\n\",\n",
    "    \"        improved_model = DistilBertForMaskedLM.from_pretrained(fine_tuned_model_path).to(device)\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        improved_model = RobertaForMaskedLM.from_pretrained(fine_tuned_model_path).to(device)\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    logger.info('Fine-tuning model with LoRA...')\\n\",\n",
    "    \"    training_args = TrainingArguments(\\n\",\n",
    "    \"        output_dir='/kaggle/working/output' if Path('/kaggle/working').exists() else './output',\\n\",\n",
    "    \"        num_train_epochs=1,\\n\",\n",
    "    \"        per_device_train_batch_size=4,\\n\",\n",
    "    \"        eval_strategy='no',\\n\",\n",
    "    \"        logging_dir='/kaggle/working/logs' if Path('/kaggle/working').exists() else './logs',\\n\",\n",
    "    \"        report_to='none',\\n\",\n",
    "    \"        fp16=True\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    if classification == 'big':\\n\",\n",
    "    \"        trainer = Trainer(\\n\",\n",
    "    \"            model=improved_model,\\n\",\n",
    "    \"            args=training_args,\\n\",\n",
    "    \"            train_dataset=tokenized_train_improved,\\n\",\n",
    "    \"            data_collator=DataCollatorForLanguageModeling(tokenizer=improved_tokenizer, mlm=True)\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        trainer.train()\\n\",\n",
    "    \"        logger.info(f'Saving fine-tuned model to {fine_tuned_model_path}')\\n\",\n",
    "    \"        improved_model.save_pretrained(fine_tuned_model_path)\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        logger.info('Dataset is small, skipping fine-tuning')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Evaluation function with focal loss and batched inference\\n\",\n",
    "    \"def evaluate_mlm(model, tokenizer, dataset, device, gamma=2.0, batch_size=16):\\n\",\n",
    "    \"    model.eval()\\n\",\n",
    "    \"    accuracy_metric = evaluate.load('accuracy')\\n\",\n",
    "    \"    f1_metric = evaluate.load('f1')\\n\",\n",
    "    \"    predictions, labels = [], []\\n\",\n",
    "    \"    perplexity_scores = []\\n\",\n",
    "    \"    latencies = []\\n\",\n",
    "    \"    memory_usages = []\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for i in range(0, len(dataset), batch_size):\\n\",\n",
    "    \"        batch = dataset[i:i+batch_size]\\n\",\n",
    "    \"        start_time = time.time()\\n\",\n",
    "    \"        input_ids = torch.tensor(batch['input_ids']).to(device)\\n\",\n",
    "    \"        attention_mask = torch.tensor(batch['attention_mask']).to(device)\\n\",\n",
    "    \"        logger.info(f\\\"Batch {i//batch_size}: input_ids shape = {input_ids.shape}, attention_mask shape = {attention_mask.shape}\\\")\\n\",\n",
    "    \"        mask_token_indices = [(input_ids[j] == tokenizer.mask_token_id).nonzero(as_tuple=True)[0] for j in range(input_ids.size(0))]\\n\",\n",
    "    \"        original_tokens = []\\n\",\n",
    "    \"        valid_mask_indices = []\\n\",\n",
    "    \"        for j in range(input_ids.size(0)):\\n\",\n",
    "    \"            if len(mask_token_indices[j]) == 0:\\n\",\n",
    "    \"                valid_indices = (input_ids[j] != tokenizer.pad_token_id) & (input_ids[j] != tokenizer.cls_token_id) & (input_ids[j] != tokenizer.sep_token_id)\\n\",\n",
    "    \"                valid_indices = valid_indices.nonzero(as_tuple=True)[0]\\n\",\n",
    "    \"                if len(valid_indices) == 0:\\n\",\n",
    "    \"                    continue\\n\",\n",
    "    \"                mask_idx = valid_indices[torch.randint(0, len(valid_indices), (1,)).item()]\\n\",\n",
    "    \"                original_token = input_ids[j, mask_idx].clone()\\n\",\n",
    "    \"                input_ids[j, mask_idx] = tokenizer.mask_token_id\\n\",\n",
    "    \"                mask_token_indices[j] = torch.tensor([mask_idx]).to(device)\\n\",\n",
    "    \"                original_tokens.append([original_token.item()])\\n\",\n",
    "    \"                valid_mask_indices.append([mask_idx])\\n\",\n",
    "    \"            else:\\n\",\n",
    "    \"                original_token = input_ids[j, mask_token_indices[j]].cpu().numpy()\\n\",\n",
    "    \"                if original_token.ndim == 0:\\n\",\n",
    "    \"                    original_token = [original_token.item()]\\n\",\n",
    "    \"                else:\\n\",\n",
    "    \"                    original_token = original_token.tolist()\\n\",\n",
    "    \"                original_tokens.append(original_token)\\n\",\n",
    "    \"                valid_mask_indices.append(mask_token_indices[j].cpu().numpy().tolist())\\n\",\n",
    "    \"        if not original_tokens:\\n\",\n",
    "    \"            logger.info(f\\\"Batch {i//batch_size}: No valid tokens, skipping\\\")\\n\",\n",
    "    \"            continue\\n\",\n",
    "    \"        with torch.no_grad():\\n\",\n",
    "    \"            outputs = model(input_ids, attention_mask=attention_mask)\\n\",\n",
    "    \"            logits = outputs.logits\\n\",\n",
    "    \"            for j in range(len(original_tokens)):\\n\",\n",
    "    \"                mask_indices = valid_mask_indices[j]\\n\",\n",
    "    \"                if not mask_indices:\\n\",\n",
    "    \"                    continue\\n\",\n",
    "    \"                predicted_token_id = torch.argmax(logits[j, mask_indices], dim=-1)\\n\",\n",
    "    \"                probs = F.softmax(logits[j, mask_indices], dim=-1)\\n\",\n",
    "    \"                log_probs = F.log_softmax(logits[j, mask_indices], dim=-1)\\n\",\n",
    "    \"                log_probs = torch.clamp(log_probs, min=-100, max=0)\\n\",\n",
    "    \"                try:\\n\",\n",
    "    \"                    neg_log_prob = -log_probs[torch.arange(len(original_tokens[j])), original_tokens[j]]\\n\",\n",
    "    \"                    if neg_log_prob.max() > 100:\\n\",\n",
    "    \"                        logger.info(f\\\"Sample {i+j}: Extreme neg_log_prob {neg_log_prob.max().item():.4f}, skipping\\\")\\n\",\n",
    "    \"                        continue\\n\",\n",
    "    \"                    perplexity = torch.exp(torch.clamp(neg_log_prob, max=100).mean())\\n\",\n",
    "    \"                    if not torch.isfinite(perplexity):\\n\",\n",
    "    \"                        logger.info(f\\\"Sample {i+j}: Non-finite perplexity, skipping\\\")\\n\",\n",
    "    \"                        continue\\n\",\n",
    "    \"                    perplexity_scores.append(perplexity.item())\\n\",\n",
    "    \"                    logger.info(f\\\"Sample {i+j}: Perplexity = {perplexity.item():.4f}, Neg log prob = {neg_log_prob.mean().item():.4f}\\\")\\n\",\n",
    "    \"                except Exception as e:\\n\",\n",
    "    \"                    logger.error(f\\\"Sample {i+j}: Perplexity error: {e}\\\")\\n\",\n",
    "    \"                    continue\\n\",\n",
    "    \"                ce_loss = F.cross_entropy(logits[j, mask_indices], torch.tensor(original_tokens[j]).to(device), reduction='none')\\n\",\n",
    "    \"                pt = torch.exp(-ce_loss)\\n\",\n",
    "    \"                focal_loss = (1 - pt) ** 2.0 * ce_loss\\n\",\n",
    "    \"                logger.info(f\\\"Sample {i+j}: Focal loss = {focal_loss.mean().item():.4f}\\\")\\n\",\n",
    "    \"                predictions.extend(predicted_token_id.cpu().numpy().tolist())\\n\",\n",
    "    \"                labels.extend(original_tokens[j])\\n\",\n",
    "    \"        latency = (time.time() - start_time) / len(original_tokens)\\n\",\n",
    "    \"        latencies.extend([latency] * len(original_tokens))\\n\",\n",
    "    \"        memory = (psutil.Process().memory_info().rss / 1024**2) - baseline_memory\\n\",\n",
    "    \"        memory_usages.extend([memory] * len(original_tokens))\\n\",\n",
    "    \"        del input_ids, attention_mask, logits, outputs\\n\",\n",
    "    \"        torch.cuda.empty_cache()\\n\",\n",
    "    \"    if not predictions:\\n\",\n",
    "    \"        logger.warning('No valid predictions; returning 0 metrics')\\n\",\n",
    "    \"        return {'accuracy': 0.0, 'f1': 0.0, 'perplexity': float('inf'), 'latency': 0.0, 'memory': 0.0}\\n\",\n",
    "    \"    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)['accuracy']\\n\",\n",
    "    \"    f1 = f1_metric.compute(predictions=predictions, references=labels, average='weighted')['f1']\\n\",\n",
    "    \"    avg_perplexity = np.mean(perplexity_scores) if perplexity_scores else float('inf')\\n\",\n",
    "    \"    avg_latency = np.mean(latencies)\\n\",\n",
    "    \"    avg_memory = np.mean(memory_usages)\\n\",\n",
    "    \"    return {'accuracy': accuracy, 'f1': f1, 'perplexity': avg_perplexity, 'latency': avg_latency, 'memory': avg_memory}\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Run evaluation with quantization\\n\",\n",
    "    \"from torch.quantization import quantize_dynamic\\n\",\n",
    "    \"logger.info(\\\"Applying dynamic quantization (qint8) for CPU backend\\\")\\n\",\n",
    "    \"base_model_quantized = quantize_dynamic(base_model.to('cpu'), {torch.nn.Linear}, dtype=torch.qint8).to(quant_device)\\n\",\n",
    "    \"improved_model_quantized = quantize_dynamic(improved_model.to('cpu'), {torch.nn.Linear}, dtype=torch.qint8).to(quant_device)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Log model size\\n\",\n",
    "    \"def get_model_size(model):\\n\",\n",
    "    \"    torch.save(model.state_dict(), \\\"temp.pt\\\")\\n\",\n",
    "    \"    size = os.path.getsize(\\\"temp.pt\\\") / 1024**2\\n\",\n",
    "    \"    os.remove(\\\"temp.pt\\\")\\n\",\n",
    "    \"    return size\\n\",\n",
    "    \"\\n\",\n",
    "    \"logger.info(f\\\"BaseBERT quantized size: {get_model_size(base_model_quantized):.2f} MB\\\")\\n\",\n",
    "    \"logger.info(f\\\"Improved quantized size: {get_model_size(improved_model_quantized):.2f} MB\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"start_time = time.time()\\n\",\n",
    "    \"base_metrics = evaluate_mlm(base_model_quantized, base_tokenizer, tokenized_val_base, quant_device, batch_size=16)\\n\",\n",
    "    \"improved_metrics = evaluate_mlm(improved_model_quantized, improved_tokenizer, tokenized_val_improved, quant_device, batch_size=16)\\n\",\n",
    "    \"eval_time = time.time() - start_time\\n\",\n",
    "    \"\\n\",\n",
    "    \"logger.info(f\\\"BaseBERT metrics: {base_metrics}\\\")\\n\",\n",
    "    \"logger.info(f\\\"Improved model metrics: {improved_metrics}\\\")\\n\",\n",
    "    \"logger.info(f\\\"Evaluation time: {eval_time:.2f} seconds\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Save results with local fallback\\n\",\n",
    "    \"output_dir = '/kaggle/working' if Path('/kaggle/working').exists() else '.'\\n\",\n",
    "    \"results = pd.DataFrame({\\n\",\n",
    "    \"    'Model': ['BaseBERT', 'Improved'],\\n\",\n",
    "    \"    'Accuracy': [base_metrics['accuracy'], improved_metrics['accuracy']],\\n\",\n",
    "    \"    'F1': [base_metrics['f1'], improved_metrics['f1']],\\n\",\n",
    "    \"    'Perplexity': [base_metrics['perplexity'], improved_metrics['perplexity']],\\n\",\n",
    "    \"    'LatencySeconds': [base_metrics['latency'], improved_metrics['latency']],\\n\",\n",
    "    \"    'MemoryMB': [base_metrics['memory'], improved_metrics['memory']],\\n\",\n",
    "    \"    'EvalTimeSeconds': [eval_time / 2, eval_time / 2]\\n\",\n",
    "    \"})\\n\",\n",
    "    \"results.to_csv(os.path.join(output_dir, 'results.csv'), index=False)\\n\",\n",
    "    \"logger.info(f\\\"Results saved: {results}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Interpret results\\n\",\n",
    "    \"threshold = 0.60\\n\",\n",
    "    \"is_good = improved_metrics['accuracy'] >= threshold\\n\",\n",
    "    \"interpretation = {\\n\",\n",
    "    \"    'status': 'Good' if is_good else 'Needs Improvement',\\n\",\n",
    "    \"    'reason': f'Improved model (CGABERT) accuracy ({improved_metrics[\\\"accuracy\\\"]:.4f}) {\\\"exceeds\\\" if is_good else \\\"is below\\\"} threshold ({threshold}) for effective text autocomplete.',\\n\",\n",
    "    \"    'dataset': {'size': num_samples, 'language': language, 'samples': sample_texts[:3]},\\n\",\n",
    "    \"    'metrics': improved_metrics\\n\",\n",
    "    \"}\\n\",\n",
    "    \"with open(os.path.join(output_dir, 'interpretation.json'), 'w') as f:\\n\",\n",
    "    \"    json.dump(interpretation, f, indent=4)\\n\",\n",
    "    \"logger.info(\\\"Interpretation saved\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"logger.info(\\\"Kernel executed successfully\\\")\\n\",\n",
    "    \"print(\\\"Execution completed. Check results.csv and interpretation.json in\\\", output_dir)\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3 (ipykernel)\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.11.0\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
